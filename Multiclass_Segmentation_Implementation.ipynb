{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPonZNdH0aOKmTEUjDccWpF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taramas73/DS-final-project/blob/irusha/Multiclass_Segmentation_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Multiclass Segmentation Implementation\n",
        "Here's how to implement multiclass segmentation with one-hot encoding and softmax output:"
      ],
      "metadata": {
        "id": "OeJqpVkPAc9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Make sure your masks are properly one-hot encoded\n",
        "*   Balance your dataset if class distributions are uneven\n",
        "*   Consider using class weights if some classes are rare but important\n",
        "*  The combined loss function (Dice + Categorical Crossentropy) usually gives better results than either alone\n",
        "\n"
      ],
      "metadata": {
        "id": "AfvzlwRjAtLE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YqwEq77lASh2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def preprocess_multiclass_masks(mask_paths, class_mapping, img_height=256, img_width=256):\n",
        "    \"\"\"\n",
        "    Preprocess masks for multiclass segmentation\n",
        "\n",
        "    Args:\n",
        "        mask_paths: List of paths to mask images\n",
        "        class_mapping: Dictionary mapping pixel values to class indices\n",
        "        img_height: Height to resize masks to\n",
        "        img_width: Width to resize masks to\n",
        "\n",
        "    Returns:\n",
        "        Preprocessed one-hot encoded masks\n",
        "    \"\"\"\n",
        "    num_classes = len(class_mapping)\n",
        "    masks = []\n",
        "\n",
        "    for mask_path in mask_paths:\n",
        "        # Load mask image (assuming it contains class indices as pixel values)\n",
        "        mask = tf.keras.preprocessing.image.load_img(\n",
        "            mask_path, target_size=(img_height, img_width), color_mode='grayscale'\n",
        "        )\n",
        "        mask = np.array(mask)\n",
        "        mask = mask.squeeze()\n",
        "\n",
        "        # Map original pixel values to class indices\n",
        "        encoded_mask = np.zeros_like(mask)\n",
        "        for original_value, class_idx in class_mapping.items():\n",
        "            encoded_mask[mask == original_value] = class_idx\n",
        "\n",
        "        # Convert to one-hot encoding\n",
        "        one_hot_mask = to_categorical(encoded_mask, num_classes=num_classes)\n",
        "        masks.append(one_hot_mask)\n",
        "\n",
        "    return np.array(masks)\n",
        "\n",
        "def multiclass_dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Dice coefficient for multiclass segmentation\n",
        "    \"\"\"\n",
        "    # Flatten the predictions and true values\n",
        "    y_true_flat = tf.reshape(y_true, [-1, tf.shape(y_true)[-1]])\n",
        "    y_pred_flat = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
        "\n",
        "    # Calculate intersection and union for each class\n",
        "    intersection = tf.reduce_sum(y_true_flat * y_pred_flat, axis=0)\n",
        "    union = tf.reduce_sum(y_true_flat, axis=0) + tf.reduce_sum(y_pred_flat, axis=0)\n",
        "\n",
        "    # Calculate dice coefficient for each class\n",
        "    dice = (2. * intersection + smooth) / (union + smooth)\n",
        "\n",
        "    # Return mean dice over all classes\n",
        "    return tf.reduce_mean(dice)\n",
        "\n",
        "def multiclass_dice_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Dice loss for multiclass segmentation\n",
        "    \"\"\"\n",
        "    return 1 - multiclass_dice_coefficient(y_true, y_pred)\n",
        "\n",
        "# Combine with categorical crossentropy for better performance\n",
        "def combined_multiclass_loss(y_true, y_pred):\n",
        "    dice_loss = multiclass_dice_loss(y_true, y_pred)\n",
        "    ce_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "    return dice_loss + ce_loss\n",
        "\n",
        "# Data generator for multiclass segmentation\n",
        "class MulticlassDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, image_paths, mask_paths, class_mapping, batch_size=8,\n",
        "                 img_height=256, img_width=256, augmentation=None, shuffle=True):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.class_mapping = class_mapping\n",
        "        self.num_classes = len(class_mapping)\n",
        "        self.batch_size = batch_size\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.augmentation = augmentation\n",
        "        self.shuffle = shuffle\n",
        "        self.indexes = np.arange(len(image_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_images = []\n",
        "        batch_masks = []\n",
        "\n",
        "        for i in batch_indexes:\n",
        "            # Load and preprocess image\n",
        "            img = tf.keras.preprocessing.image.load_img(\n",
        "                self.image_paths[i], target_size=(self.img_height, self.img_width)\n",
        "            )\n",
        "            img = np.array(img) / 255.0\n",
        "\n",
        "            # Load and preprocess mask\n",
        "            mask = tf.keras.preprocessing.image.load_img(\n",
        "                self.mask_paths[i], target_size=(self.img_height, self.img_width),\n",
        "                color_mode='grayscale'\n",
        "            )\n",
        "            mask = np.array(mask)\n",
        "            mask = mask.squeeze()\n",
        "\n",
        "            # Map original pixel values to class indices\n",
        "            encoded_mask = np.zeros_like(mask)\n",
        "            for original_value, class_idx in self.class_mapping.items():\n",
        "                encoded_mask[mask == original_value] = class_idx\n",
        "\n",
        "            # Convert to one-hot encoding\n",
        "            one_hot_mask = to_categorical(encoded_mask, num_classes=self.num_classes)\n",
        "\n",
        "            # Apply augmentation if specified\n",
        "            if self.augmentation:\n",
        "                augmented = self.augmentation(image=img, mask=one_hot_mask)\n",
        "                img = augmented['image']\n",
        "                one_hot_mask = augmented['mask']\n",
        "\n",
        "            batch_images.append(img)\n",
        "            batch_masks.append(one_hot_mask)\n",
        "\n",
        "        return np.array(batch_images), np.array(batch_masks)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "# Example of setting up and training a multiclass segmentation model\n",
        "def train_multiclass_model(train_img_paths, train_mask_paths,\n",
        "                           val_img_paths, val_mask_paths,\n",
        "                           class_mapping, batch_size=8, epochs=50):\n",
        "    # Create data generators\n",
        "    train_gen = MulticlassDataGenerator(\n",
        "        train_img_paths, train_mask_paths, class_mapping, batch_size=batch_size\n",
        "    )\n",
        "    val_gen = MulticlassDataGenerator(\n",
        "        val_img_paths, val_mask_paths, class_mapping, batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    # Create model - you can use the EfficientNet + U-Net hybrid or regular U-Net\n",
        "    # The only difference is the output layer should use softmax and output num_classes channels\n",
        "    model = build_efficient_unet(input_shape=(256, 256, 3), n_classes=len(class_mapping))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss=combined_multiclass_loss,\n",
        "        metrics=[multiclass_dice_coefficient, 'categorical_accuracy']\n",
        "    )\n",
        "\n",
        "    # Setup callbacks\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'best_multiclass_model.h5',\n",
        "            save_best_only=True,\n",
        "            monitor='val_multiclass_dice_coefficient',\n",
        "            mode='max'\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6\n",
        "        ),\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss', patience=15, restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    return model, history"
      ]
    }
  ]
}