{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxpIbjpI/lsdTfo86OZzN/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taramas73/DS-final-project/blob/irusha/Segment_Anything_Model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Working with Meta's SAM Model\n",
        "For implementing SAM, you can use the Segment Anything Model from Meta:"
      ],
      "metadata": {
        "id": "BQ4es1O49LO_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCUQQMaY9FqV",
        "outputId": "0eee0551-3bd6-4959-9e28-927ea1b2218b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-9qfka2nz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-9qfka2nz\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment_anything\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment_anything: filename=segment_anything-1.0-py3-none-any.whl size=36592 sha256=e8ea1fa3b107d1878f4e80148ba9cc87fa28cf9db81d29386af14053fe169874\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_uyfft3p/wheels/15/d7/bd/05f5f23b7dcbe70cbc6783b06f12143b0cf1a5da5c7b52dcc5\n",
            "Successfully built segment_anything\n",
            "Installing collected packages: segment_anything\n",
            "Successfully installed segment_anything-1.0\n"
          ]
        }
      ],
      "source": [
        "# First install the segment-anything package\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "def setup_sam():\n",
        "    # Download the model checkpoint\n",
        "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "    # Load the SAM model\n",
        "    sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "    model_type = \"vit_h\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "    sam.to(device=device)\n",
        "\n",
        "    return SamPredictor(sam)\n",
        "\n",
        "def segment_image(predictor, image, points=None, boxes=None):\n",
        "    \"\"\"\n",
        "    Use SAM to segment an image\n",
        "    - points: Optional list of points to guide segmentation, shape Nx2\n",
        "    - boxes: Optional list of boxes to guide segmentation, shape Nx4\n",
        "    \"\"\"\n",
        "    predictor.set_image(image)\n",
        "\n",
        "    if points is not None:\n",
        "        # Convert points to numpy arrays if they aren't already\n",
        "        input_points = np.array(points)\n",
        "        input_labels = np.ones(len(points))  # 1 for foreground points\n",
        "        masks, scores, logits = predictor.predict(\n",
        "            point_coords=input_points,\n",
        "            point_labels=input_labels,\n",
        "            multimask_output=True\n",
        "        )\n",
        "    elif boxes is not None:\n",
        "        # Convert boxes to numpy arrays if they aren't already\n",
        "        input_boxes = np.array(boxes)\n",
        "        masks, scores, logits = predictor.predict(\n",
        "            box=input_boxes[0],  # taking the first box if multiple are provided\n",
        "            multimask_output=True\n",
        "        )\n",
        "    else:\n",
        "        # Automatic segmentation based on image features\n",
        "        masks, scores, logits = predictor.predict(\n",
        "            multimask_output=True\n",
        "        )\n",
        "\n",
        "    return masks, scores, logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* SAM requires significant compute resources, especially for the larger models\n",
        "* Consider using interactive prompts for better segmentation results\n",
        "\n",
        "* SAM can be used both for generating training data and for inference\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DJWLuska92fj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7XLRGp6Z-hKP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}