{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzZMpNztEVwVuwsEQxeU6O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taramas73/DS-final-project/blob/irusha/Projet_Artefact.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RAzkCaPU7Qf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After analyzing the options, I believe implementing cross-attention mechanisms between pre and post-disaster images would provide the most significant improvement to the existing architecture. This approach will help the model better identify and focus on the relevant changes between images, which is crucial for accurate damage assessment.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet34, ResNet34_Weights\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Cross-attention module to help the model focus on relevant changes\n",
        "    between pre and post disaster images.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        \n",
        "    def forward(self, pre_features, post_features):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "            pre_features: features from pre-disaster image [B, C, H, W]\n",
        "            post_features: features from post-disaster image [B, C, H, W]\n",
        "        \"\"\"\n",
        "        batch_size, C, height, width = pre_features.size()\n",
        "        \n",
        "        # Pre features generate queries\n",
        "        proj_query = self.query_conv(pre_features).view(batch_size, -1, height * width).permute(0, 2, 1)\n",
        "        \n",
        "        # Post features generate keys\n",
        "        proj_key = self.key_conv(post_features).view(batch_size, -1, height * width)\n",
        "        \n",
        "        # Attention map\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = self.softmax(energy)\n",
        "        \n",
        "        # Post features generate values\n",
        "        proj_value = self.value_conv(post_features).view(batch_size, -1, height * width)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, height, width)\n",
        "        \n",
        "        # Residual connection with pre_features\n",
        "        out = self.gamma * out + pre_features\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder block for Unet-like architecture\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, middle_channels, out_channels):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.decode = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(middle_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(middle_channels, middle_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(middle_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.decode(x)\n",
        "\n",
        "\n",
        "class SiameseNetworkWithCrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Siamese Network with Cross-Attention for building damage assessment\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=5, pretrained=True):\n",
        "        super(SiameseNetworkWithCrossAttention, self).__init__()\n",
        "        \n",
        "        # Load pretrained ResNet34 backbone\n",
        "        weights = ResNet34_Weights.DEFAULT if pretrained else None\n",
        "        encoder = resnet34(weights=weights)\n",
        "        \n",
        "        # Define encoder stages\n",
        "        self.enc1 = nn.Sequential(encoder.conv1, encoder.bn1, encoder.relu)  # 64 channels\n",
        "        self.enc2 = nn.Sequential(encoder.maxpool, encoder.layer1)           # 64 channels\n",
        "        self.enc3 = encoder.layer2                                           # 128 channels\n",
        "        self.enc4 = encoder.layer3                                           # 256 channels\n",
        "        self.enc5 = encoder.layer4                                           # 512 channels\n",
        "        \n",
        "        # Cross-attention modules\n",
        "        self.ca1 = CrossAttention(64)\n",
        "        self.ca2 = CrossAttention(64)\n",
        "        self.ca3 = CrossAttention(128)\n",
        "        self.ca4 = CrossAttention(256)\n",
        "        self.ca5 = CrossAttention(512)\n",
        "        \n",
        "        # Decoder stages with skip connections\n",
        "        self.dec5 = DecoderBlock(512, 512, 256)\n",
        "        self.dec4 = DecoderBlock(512, 256, 128)\n",
        "        self.dec3 = DecoderBlock(256, 128, 64)\n",
        "        self.dec2 = DecoderBlock(128, 64, 64)\n",
        "        self.dec1 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        \n",
        "        # Final classification layer\n",
        "        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "        \n",
        "    def forward(self, pre_img, post_img):\n",
        "        \"\"\"\n",
        "        Forward pass with pre and post disaster images\n",
        "        Args:\n",
        "            pre_img: pre-disaster image [B, 3, H, W]\n",
        "            post_img: post-disaster image [B, 3, H, W]\n",
        "        \"\"\"\n",
        "        # Encode pre-disaster image\n",
        "        pre_enc1 = self.enc1(pre_img)                 # [B, 64, H/2, W/2]\n",
        "        pre_enc2 = self.enc2(pre_enc1)                # [B, 64, H/4, W/4]\n",
        "        pre_enc3 = self.enc3(pre_enc2)                # [B, 128, H/8, W/8]\n",
        "        pre_enc4 = self.enc4(pre_enc3)                # [B, 256, H/16, W/16]\n",
        "        pre_enc5 = self.enc5(pre_enc4)                # [B, 512, H/32, W/32]\n",
        "        \n",
        "        # Encode post-disaster image\n",
        "        post_enc1 = self.enc1(post_img)               # [B, 64, H/2, W/2]\n",
        "        post_enc2 = self.enc2(post_enc1)              # [B, 64, H/4, W/4]\n",
        "        post_enc3 = self.enc3(post_enc2)              # [B, 128, H/8, W/8]\n",
        "        post_enc4 = self.enc4(post_enc3)              # [B, 256, H/16, W/16]\n",
        "        post_enc5 = self.enc5(post_enc4)              # [B, 512, H/32, W/32]\n",
        "        \n",
        "        # Apply cross-attention at each level\n",
        "        ca_enc1 = self.ca1(pre_enc1, post_enc1)\n",
        "        ca_enc2 = self.ca2(pre_enc2, post_enc2)\n",
        "        ca_enc3 = self.ca3(pre_enc3, post_enc3)\n",
        "        ca_enc4 = self.ca4(pre_enc4, post_enc4)\n",
        "        ca_enc5 = self.ca5(pre_enc5, post_enc5)\n",
        "        \n",
        "        # Decode with skip connections\n",
        "        dec5 = self.dec5(ca_enc5)                                      # [B, 256, H/16, W/16]\n",
        "        dec4 = self.dec4(torch.cat([dec5, ca_enc4], dim=1))            # [B, 128, H/8, W/8]\n",
        "        dec3 = self.dec3(torch.cat([dec4, ca_enc3], dim=1))            # [B, 64, H/4, W/4]\n",
        "        dec2 = self.dec2(torch.cat([dec3, ca_enc2], dim=1))            # [B, 64, H/2, W/2]\n",
        "        dec1 = self.dec1(torch.cat([dec2, ca_enc1], dim=1))            # [B, 64, H/2, W/2]\n",
        "        \n",
        "        # Final classification\n",
        "        outputs = self.final(dec1)                                     # [B, num_classes, H/2, W/2]\n",
        "        \n",
        "        # Upscale to original image size\n",
        "        outputs = F.interpolate(outputs, size=pre_img.shape[2:], mode='bilinear', align_corners=False)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Focal Loss to handle class imbalance better\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2, weight=None, ignore_index=255):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight\n",
        "        self.ignore_index = ignore_index\n",
        "        self.ce_fn = nn.CrossEntropyLoss(weight=self.weight, ignore_index=self.ignore_index)\n",
        "\n",
        "    def forward(self, preds, labels):\n",
        "        logpt = -self.ce_fn(preds, labels)\n",
        "        pt = torch.exp(logpt)\n",
        "        loss = -((1 - pt) ** self.gamma) * self.alpha * logpt\n",
        "        return loss\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=100, device='cuda'):\n",
        "    \"\"\"\n",
        "    Training loop for the model\n",
        "    \"\"\"\n",
        "    best_score = 0.0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        for batch_idx, (pre_imgs, post_imgs, targets) in enumerate(train_loader):\n",
        "            pre_imgs = pre_imgs.to(device)\n",
        "            post_imgs = post_imgs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            \n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward\n",
        "            outputs = model(pre_imgs, post_imgs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            # Backward + optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            if batch_idx % 20 == 19:\n",
        "                print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx+1}, Loss: {running_loss/20:.4f}')\n",
        "                running_loss = 0.0\n",
        "                \n",
        "        # Validate after each epoch\n",
        "        val_score = validate_model(model, val_loader, device)\n",
        "        print(f'Epoch: {epoch+1}/{num_epochs}, Validation F1 Score: {val_score:.4f}')\n",
        "        \n",
        "        # Save best model\n",
        "        if val_score > best_score:\n",
        "            best_score = val_score\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            \n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "        \n",
        "    return model\n",
        "\n",
        "\n",
        "# Validation function to calculate F1 score\n",
        "def validate_model(model, val_loader, device='cuda'):\n",
        "    \"\"\"\n",
        "    Validation function that calculates F1 scores for building localization and damage classification\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Initialize metrics\n",
        "    tp_loc, fp_loc, fn_loc = 0, 0, 0\n",
        "    tp_class, fp_class, fn_class = 0, 0, 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for pre_imgs, post_imgs, targets in val_loader:\n",
        "            pre_imgs = pre_imgs.to(device)\n",
        "            post_imgs = post_imgs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            \n",
        "            outputs = model(pre_imgs, post_imgs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            \n",
        "            # Building localization (any class > 0 is a building)\n",
        "            pred_buildings = (preds > 0)\n",
        "            target_buildings = (targets > 0)\n",
        "            \n",
        "            tp_loc += torch.logical_and(pred_buildings, target_buildings).sum().item()\n",
        "            fp_loc += torch.logical_and(pred_buildings, ~target_buildings).sum().item()\n",
        "            fn_loc += torch.logical_and(~pred_buildings, target_buildings).sum().item()\n",
        "            \n",
        "            # Damage classification (only for correctly detected buildings)\n",
        "            building_mask = (targets > 0)\n",
        "            correct_buildings = torch.logical_and(pred_buildings, target_buildings)\n",
        "            \n",
        "            for c in range(1, 5):  # Damage classes (1-4)\n",
        "                pred_c = (preds == c)\n",
        "                target_c = (targets == c)\n",
        "                \n",
        "                tp_class += torch.logical_and(pred_c, target_c).sum().item()\n",
        "                fp_class += torch.logical_and(pred_c, ~target_c).sum().item()\n",
        "                fn_class += torch.logical_and(~pred_c, target_c).sum().item()\n",
        "    \n",
        "    # Calculate F1 scores\n",
        "    precision_loc = tp_loc / (tp_loc + fp_loc + 1e-8)\n",
        "    recall_loc = tp_loc / (tp_loc + fn_loc + 1e-8)\n",
        "    f1_loc = 2 * precision_loc * recall_loc / (precision_loc + recall_loc + 1e-8)\n",
        "    \n",
        "    precision_class = tp_class / (tp_class + fp_class + 1e-8)\n",
        "    recall_class = tp_class / (tp_class + fn_class + 1e-8)\n",
        "    f1_class = 2 * precision_class * recall_class / (precision_class + recall_class + 1e-8)\n",
        "    \n",
        "    # Calculate weighted score (0.3*F1_loc + 0.7*F1_class)\n",
        "    weighted_score = 0.3 * f1_loc + 0.7 * f1_class\n",
        "    \n",
        "    return weighted_score\n",
        "\n",
        "\n",
        "# Example of how to use the model\n",
        "def main():\n",
        "    # Initialize model\n",
        "    model = SiameseNetworkWithCrossAttention(num_classes=5)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Define loss function with class weights to handle imbalance\n",
        "    class_weights = torch.tensor([1.0, 1.0, 3.0, 3.0, 3.0], device=device)\n",
        "    criterion = FocalLoss(weight=class_weights)\n",
        "    \n",
        "    # Define optimizer and scheduler\n",
        "    optimizer = torch.optim.RAdam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n",
        "    \n",
        "    # Train and validate (assuming you have data loaders)\n",
        "    # train_model(model, train_loader, val_loader, criterion, optimizer, scheduler)\n",
        "    \n",
        "    print(\"Model training code is ready to be executed with your data loaders.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "```\n",
        "\n",
        "This improved implementation adds cross-attention mechanisms to the Siamese network architecture, which should significantly enhance the model's ability to detect and classify building damage. Here's what makes this implementation powerful:\n",
        "\n",
        "### Key Improvements\n",
        "\n",
        "1. **Cross-Attention Mechanism**:\n",
        "   - The model now uses queries from pre-disaster features and keys/values from post-disaster features\n",
        "   - This helps the network focus on relevant changes between the images\n",
        "   - The attention is applied at multiple levels of the encoder, allowing multi-scale change detection\n",
        "\n",
        "2. **Enhanced Loss Function**:\n",
        "   - Implemented Focal Loss instead of weighted Cross-Entropy\n",
        "   - Better handles class imbalance by focusing more on hard examples and less on easy ones\n",
        "   - Maintains the class weighting to prioritize under-represented damage classes\n",
        "\n",
        "3. **ResNet34 Backbone with Skip Connections**:\n",
        "   - Uses a proven encoder with pretrained weights\n",
        "   - Detailed skip connections ensure high-resolution feature preservation\n",
        "   - Maintains spatial information critical for accurate building segmentation\n",
        "\n",
        "4. **Comprehensive Training and Validation Functions**:\n",
        "   - Includes complete training loop with learning rate scheduling\n",
        "   - Validation function calculates the competition-specific weighted F1 scores\n",
        "   - Best model checkpointing based on validation performance\n",
        "\n",
        "### Usage\n",
        "\n",
        "To use this model, you would:\n",
        "\n",
        "1. Prepare your data loaders for the xBD dataset\n",
        "2. Initialize the model, loss function, optimizer, and scheduler\n",
        "3. Run the training loop, which handles both training and validation\n",
        "4. Load the best model for inference\n",
        "\n",
        "The cross-attention mechanism should provide better focus on the changes between pre and post-disaster images, leading to more accurate damage classification while maintaining good building localization performance."
      ],
      "metadata": {
        "id": "cgF2168d7dc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet34, ResNet34_Weights\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Cross-attention module to help the model focus on relevant changes\n",
        "    between pre and post disaster images.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, pre_features, post_features):\n",
        "        \"\"\"\n",
        "        inputs:\n",
        "            pre_features: features from pre-disaster image [B, C, H, W]\n",
        "            post_features: features from post-disaster image [B, C, H, W]\n",
        "        \"\"\"\n",
        "        batch_size, C, height, width = pre_features.size()\n",
        "\n",
        "        # Pre features generate queries\n",
        "        proj_query = self.query_conv(pre_features).view(batch_size, -1, height * width).permute(0, 2, 1)\n",
        "\n",
        "        # Post features generate keys\n",
        "        proj_key = self.key_conv(post_features).view(batch_size, -1, height * width)\n",
        "\n",
        "        # Attention map\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = self.softmax(energy)\n",
        "\n",
        "        # Post features generate values\n",
        "        proj_value = self.value_conv(post_features).view(batch_size, -1, height * width)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, height, width)\n",
        "\n",
        "        # Residual connection with pre_features\n",
        "        out = self.gamma * out + pre_features\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder block for Unet-like architecture\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, middle_channels, out_channels):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.decode = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(middle_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(middle_channels, middle_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(middle_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decode(x)\n",
        "\n",
        "\n",
        "class SiameseNetworkWithCrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Siamese Network with Cross-Attention for building damage assessment\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=5, pretrained=True):\n",
        "        super(SiameseNetworkWithCrossAttention, self).__init__()\n",
        "\n",
        "        # Load pretrained ResNet34 backbone\n",
        "        weights = ResNet34_Weights.DEFAULT if pretrained else None\n",
        "        encoder = resnet34(weights=weights)\n",
        "\n",
        "        # Define encoder stages\n",
        "        self.enc1 = nn.Sequential(encoder.conv1, encoder.bn1, encoder.relu)  # 64 channels\n",
        "        self.enc2 = nn.Sequential(encoder.maxpool, encoder.layer1)           # 64 channels\n",
        "        self.enc3 = encoder.layer2                                           # 128 channels\n",
        "        self.enc4 = encoder.layer3                                           # 256 channels\n",
        "        self.enc5 = encoder.layer4                                           # 512 channels\n",
        "\n",
        "        # Cross-attention modules\n",
        "        self.ca1 = CrossAttention(64)\n",
        "        self.ca2 = CrossAttention(64)\n",
        "        self.ca3 = CrossAttention(128)\n",
        "        self.ca4 = CrossAttention(256)\n",
        "        self.ca5 = CrossAttention(512)\n",
        "\n",
        "        # Decoder stages with skip connections\n",
        "        self.dec5 = DecoderBlock(512, 512, 256)\n",
        "        self.dec4 = DecoderBlock(512, 256, 128)\n",
        "        self.dec3 = DecoderBlock(256, 128, 64)\n",
        "        self.dec2 = DecoderBlock(128, 64, 64)\n",
        "        self.dec1 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Final classification layer\n",
        "        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, pre_img, post_img):\n",
        "        \"\"\"\n",
        "        Forward pass with pre and post disaster images\n",
        "        Args:\n",
        "            pre_img: pre-disaster image [B, 3, H, W]\n",
        "            post_img: post-disaster image [B, 3, H, W]\n",
        "        \"\"\"\n",
        "        # Encode pre-disaster image\n",
        "        pre_enc1 = self.enc1(pre_img)                 # [B, 64, H/2, W/2]\n",
        "        pre_enc2 = self.enc2(pre_enc1)                # [B, 64, H/4, W/4]\n",
        "        pre_enc3 = self.enc3(pre_enc2)                # [B, 128, H/8, W/8]\n",
        "        pre_enc4 = self.enc4(pre_enc3)                # [B, 256, H/16, W/16]\n",
        "        pre_enc5 = self.enc5(pre_enc4)                # [B, 512, H/32, W/32]\n",
        "\n",
        "        # Encode post-disaster image\n",
        "        post_enc1 = self.enc1(post_img)               # [B, 64, H/2, W/2]\n",
        "        post_enc2 = self.enc2(post_enc1)              # [B, 64, H/4, W/4]\n",
        "        post_enc3 = self.enc3(post_enc2)              # [B, 128, H/8, W/8]\n",
        "        post_enc4 = self.enc4(post_enc3)              # [B, 256, H/16, W/16]\n",
        "        post_enc5 = self.enc5(post_enc4)              # [B, 512, H/32, W/32]\n",
        "\n",
        "        # Apply cross-attention at each level\n",
        "        ca_enc1 = self.ca1(pre_enc1, post_enc1)\n",
        "        ca_enc2 = self.ca2(pre_enc2, post_enc2)\n",
        "        ca_enc3 = self.ca3(pre_enc3, post_enc3)\n",
        "        ca_enc4 = self.ca4(pre_enc4, post_enc4)\n",
        "        ca_enc5 = self.ca5(pre_enc5, post_enc5)\n",
        "\n",
        "        # Decode with skip connections\n",
        "        dec5 = self.dec5(ca_enc5)                                      # [B, 256, H/16, W/16]\n",
        "        dec4 = self.dec4(torch.cat([dec5, ca_enc4], dim=1))            # [B, 128, H/8, W/8]\n",
        "        dec3 = self.dec3(torch.cat([dec4, ca_enc3], dim=1))            # [B, 64, H/4, W/4]\n",
        "        dec2 = self.dec2(torch.cat([dec3, ca_enc2], dim=1))            # [B, 64, H/2, W/2]\n",
        "        dec1 = self.dec1(torch.cat([dec2, ca_enc1], dim=1))            # [B, 64, H/2, W/2]\n",
        "\n",
        "        # Final classification\n",
        "        outputs = self.final(dec1)                                     # [B, num_classes, H/2, W/2]\n",
        "\n",
        "        # Upscale to original image size\n",
        "        outputs = F.interpolate(outputs, size=pre_img.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Focal Loss to handle class imbalance better\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2, weight=None, ignore_index=255):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight\n",
        "        self.ignore_index = ignore_index\n",
        "        self.ce_fn = nn.CrossEntropyLoss(weight=self.weight, ignore_index=self.ignore_index)\n",
        "\n",
        "    def forward(self, preds, labels):\n",
        "        logpt = -self.ce_fn(preds, labels)\n",
        "        pt = torch.exp(logpt)\n",
        "        loss = -((1 - pt) ** self.gamma) * self.alpha * logpt\n",
        "        return loss\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=100, device='cuda'):\n",
        "    \"\"\"\n",
        "    Training loop for the model\n",
        "    \"\"\"\n",
        "    best_score = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_idx, (pre_imgs, post_imgs, targets) in enumerate(train_loader):\n",
        "            pre_imgs = pre_imgs.to(device)\n",
        "            post_imgs = post_imgs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward\n",
        "            outputs = model(pre_imgs, post_imgs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward + optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 20 == 19:\n",
        "                print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx+1}, Loss: {running_loss/20:.4f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Validate after each epoch\n",
        "        val_score = validate_model(model, val_loader, device)\n",
        "        print(f'Epoch: {epoch+1}/{num_epochs}, Validation F1 Score: {val_score:.4f}')\n",
        "\n",
        "        # Save best model\n",
        "        if val_score > best_score:\n",
        "            best_score = val_score\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Validation function to calculate F1 score\n",
        "def validate_model(model, val_loader, device='cuda'):\n",
        "    \"\"\"\n",
        "    Validation function that calculates F1 scores for building localization and damage classification\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize metrics\n",
        "    tp_loc, fp_loc, fn_loc = 0, 0, 0\n",
        "    tp_class, fp_class, fn_class = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for pre_imgs, post_imgs, targets in val_loader:\n",
        "            pre_imgs = pre_imgs.to(device)\n",
        "            post_imgs = post_imgs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(pre_imgs, post_imgs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            # Building localization (any class > 0 is a building)\n",
        "            pred_buildings = (preds > 0)\n",
        "            target_buildings = (targets > 0)\n",
        "\n",
        "            tp_loc += torch.logical_and(pred_buildings, target_buildings).sum().item()\n",
        "            fp_loc += torch.logical_and(pred_buildings, ~target_buildings).sum().item()\n",
        "            fn_loc += torch.logical_and(~pred_buildings, target_buildings).sum().item()\n",
        "\n",
        "            # Damage classification (only for correctly detected buildings)\n",
        "            building_mask = (targets > 0)\n",
        "            correct_buildings = torch.logical_and(pred_buildings, target_buildings)\n",
        "\n",
        "            for c in range(1, 5):  # Damage classes (1-4)\n",
        "                pred_c = (preds == c)\n",
        "                target_c = (targets == c)\n",
        "\n",
        "                tp_class += torch.logical_and(pred_c, target_c).sum().item()\n",
        "                fp_class += torch.logical_and(pred_c, ~target_c).sum().item()\n",
        "                fn_class += torch.logical_and(~pred_c, target_c).sum().item()\n",
        "\n",
        "    # Calculate F1 scores\n",
        "    precision_loc = tp_loc / (tp_loc + fp_loc + 1e-8)\n",
        "    recall_loc = tp_loc / (tp_loc + fn_loc + 1e-8)\n",
        "    f1_loc = 2 * precision_loc * recall_loc / (precision_loc + recall_loc + 1e-8)\n",
        "\n",
        "    precision_class = tp_class / (tp_class + fp_class + 1e-8)\n",
        "    recall_class = tp_class / (tp_class + fn_class + 1e-8)\n",
        "    f1_class = 2 * precision_class * recall_class / (precision_class + recall_class + 1e-8)\n",
        "\n",
        "    # Calculate weighted score (0.3*F1_loc + 0.7*F1_class)\n",
        "    weighted_score = 0.3 * f1_loc + 0.7 * f1_class\n",
        "\n",
        "    return weighted_score\n",
        "\n",
        "\n",
        "# Example of how to use the model\n",
        "def main():\n",
        "    # Initialize model\n",
        "    model = SiameseNetworkWithCrossAttention(num_classes=5)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Define loss function with class weights to handle imbalance\n",
        "    class_weights = torch.tensor([1.0, 1.0, 3.0, 3.0, 3.0], device=device)\n",
        "    criterion = FocalLoss(weight=class_weights)\n",
        "\n",
        "    # Define optimizer and scheduler\n",
        "    optimizer = torch.optim.RAdam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n",
        "\n",
        "    # Train and validate (assuming you have data loaders)\n",
        "    # train_model(model, train_loader, val_loader, criterion, optimizer, scheduler)\n",
        "\n",
        "    print(\"Model training code is ready to be executed with your data loaders.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "7nOlXC2p1k6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "052dece2-166d-43f8-fed5-31ff57dca135"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:01<00:00, 84.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training code is ready to be executed with your data loaders.\n"
          ]
        }
      ]
    }
  ]
}